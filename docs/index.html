<!DOCTYPE html>
<html lang="en"><head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Computer Vision Class Project
  | Georgia Tech | Fall 2018: CS 6476 </title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">


<!-- Le styles -->
  <link href="css/bootstrap.css" rel="stylesheet">
<style>
body {
padding-top: 60px;
}
.vis {
color: #3366CC;
}
.data {
color: #FF9900;
}
</style>

<link href="css/bootstrap-responsive.min.css" rel="stylesheet">
</head>

<body>
<div class="container">
<div class="page-header">



<!-- Title and Name -->
<h1>PetSwap</h1>
<span style="font-size: 20px; line-height: 1.5em;"><strong>Hemanth Chittanuru (hchittanuru3),
  Kenny Scharm (kscharm3), Raghav Raj Mittal (rmittal34), Sarah Li (sli469)</strong></span><br>
<span style="font-size: 18px; line-height: 1.5em;">CS 6476 Computer Vision - Class Project</span><br>
<span style="font-size: 18px; line-height: 1.5em;">Georgia Tech, Fall 2019</span>
<hr>

The code for this project can be found in <a href="https://github.com/raghavrajmittal/PetSwap">this repository</a>.
<br><br>
Previous project deliverables: <a href="proposal.html">proposal</a>, <a href="update1.html">update 1</a>


 <!--It has been adapted from the following open source repositories:
<ul>
  <li><a href="https://github.com/matterport/Mask_RCNN">Mask R-CNN official implementation</a></li>
</ul>-->


<br><br>
<!-- Goal -->
<h2>Abstract</h2>
The purpose of this project is to match images of dogs with images of cats (and vice versa) based on color and texture. For example, given an input image of a cat, we wish to find an image of a dog from our dataset that it most similar in terms of fur/skin color and pattern. Our architecture consists of using Mask R-CNN to segment the animal from the image, obtaining a fused feature representation of color and texture features and clustering the images based on similarity. The expected input is a real color image of a dog or a cat. The desired output is an image of the opposite animal type with the most similar fur/skin color and pattern. We evaluate multiple clustering techniques such as k-means, mean shift, and Gaussian mixture model (GMMs) to see which produces the best results.



<!-- figure -->
<!-- Teaser Figure -->
<br><br><br>
<div style="text-align: center;">
<img style="height: 200px;" alt="" src="img/teaser_figure.png">
<br>
Sample input-output pair
</div>





<!-- <br><br> -->
<!-- Introduction -->
<h2>Introduction</h2>
We were inspired to do this project by this <a href="https://twistedsifter.com/2016/09/kittens-and-their-matching-bunnies/">article</a>. This is an application of <a href='https://en.wikipedia.org/wiki/Content-based_image_retrieval'>content-based image retrieval</a>. We are performing image retrieval based on color and texture, which is common practice in CBIR research. Our approach differs from previous work in the juxtaposition of image segmentation and CBIR. We aim to see how well this combined approach performs. This project is an interesting and fun application of various computer vision techniques.



<br><br><br>
<!-- figure -->
<br><br>
<div style="text-align: center;">
<img style="height: 350px;" alt="" src="img/architecture.png">
<br>
PetSwap Pipeline
</div>

<!-- Approach -->
<h2>Approach</h2>
<h4>Summary</h4>
Our system contains three distinct components: animal image segmentation, color/texture representation, and clustering. With these three steps we are able to take any input image of a dog or a cat and output an image with the most similar color and texture. We can simply query our pre-trained clusters with the color/texture representation of the image and return the closest image. We test various clustering techniques to see which one performs the best.
<br><br>
<h4>Animal Image Segmentation</h4>
We use <a href='https://github.com/matterport/Mask_RCNN'>Mask R-CNN</a> to segment the animal out of the image. This model has already been pre-trained on the COCO dataset, which contains color images and segmentation masks of dogs and cats (among many other objects). We used this Mask R-CNN model to segment the animals out of our two datasets (see Datasets subsection under Experiments and Results). The next step is to feed the segmented animals into our texture/color histogram generator and map each feature representation to the image using a dictionary.
<br><br>
<h4>Color/Texture Representation</h4>
Once we produce the segmented animal image, we extract color and texture features. We use this color/texture combination to generate a 24-dimensional feature representation. First, we find the mean and standard deviation of the red, green and blue channels of the image. Second, we convert the image's color space to <a href="https://en.wikipedia.org/wiki/YCbCr">YCbCr</a> and compute the mean and standard deviation across the channels to obtain another feature. Lastly, we calculate a histogram of the HSV color space.
For the texture features, we use a Gray Level co-occurence matrix, which is characterization of the texture of the image. Using this matrix, we extract statistical measures to use as features.
<br><br>
<h4>Clustering</h4>
We cluster images of dogs and cats together by feature vector similarity, where each data point has a dog or cat label. Once we cluster our training data from the dog and cat datasets, we evaluate the performance by testing it with our test sets (see Datasets subsection). When there is a query to our system, we first predict the closest cluster center. Once we are within the cluster, we then select the closest feature vector. Doing so will reduce the time complexity of the algorithm, as it removes the need to compare the query feature vector with that of every image in the dataset to find the closest match. The closest image of the opposite animal type is then returned to the user.
We qualitatively evaluated three different clustering techniques--namely k-means, mean shift, and GMM--by testing multiple images of both animal types (see Experiments and Results).



<br><br><br>
<!-- Results -->
<h2>Experiments and Results</h2>
<br>
<h3>Experimental Setup</h3>
<br>
<h4>Datasets</h4>
We use two main datasets--one containing images of cats and the other containing images of dogs. The <a href="https://archive.org/details/CAT_DATASET">Cat Dataset</a> includes 10,000 cat images and the <a href="http://vision.stanford.edu/aditya86/ImageNetDogs/">Stanford Dogs Dataset</a> includes over 20,000 images of dogs, classified by breed.
Before using these datasets to train our clusters, we first pre-processed the data. This involved extracting images from the dog and cat datasets, while also ignoring other metadata files. The dog dataset was manually shuffled so that the images were no longer separated by breed.
We divided each animal dataset into a 90% "training" and 10% testing split. In this case, the training data is the pre-clustered images. The data is now prepared for the Experimental Setup.
<br><br>As a side note, we use a pre-trained Mask R-CNN model for image segmentation. This is trained on the <a href='http://cocodataset.org'>COCO dataset</a>.
<br><br>
<h4>Existing Code</h4>
We rely heavily on the following python libraries: <a href="https://scikit-image.org/">scikit-image</a> and <a href="https://opencv.org/">OpenCV</a>.
Clustering algorithms:
<ul>
  <li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html">k-means</a></li>
  <li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MeanShift.html">mean shift</a></li>
  <li><a href="https://scikit-learn.org/stable/modules/mixture.html">GMM</a></li>
</ul>
In addition to these libraries, we use code from several GitHub repositories:
<ul>
  <li><a href="https://github.com/matterport/Mask_RCNN">Mask R-CNN</a></li>
  <li><a href="https://github.com/pochih/CBIR">CBIR techniques</a></li>
</ul>

<br>
<h4>Implementation Details</h4>
We implemented the pipeline to solve the problem end-to-end. Specifically, we created a useful feature representation that captures the key color and texture information about the images.

<br><br>
<h4>Defining Success</h4>
Quantitatively, we define success for the project to be the elbow of the k vs SSE curve and having a distance of 10% (of the mean distance) between input and output images. We also qualitatively evaluate how close the dogs and cats look by have multiple people use our system and provide feedback.

<h4>Hypertuning</h4>
<bold>k-means:</bold>
<br>
<bold>Gaussian mixture model:</bold>
<br>
<bold>Mean shift:</bold>

<br><br>
<h4>Experiments</h4>
<h5>Mask R-CNN</h5>
We hypothesized that Mask R-CNN would be accurate when segmenting dog and cat images from the other datasets. Since the Mask R-CNN model was pre-trained on COCO, it was effective in accurately segmenting out cat and dog images. Using the demo code from the original author of Mask R-CNN, we were able to successfully segment out objects in custom images.
We processed the output futher to remove bounding boxes and segmentation masks that did not correspond to the dog and cat classes. We then extract pixels corresponing to the remaining masks, and use these pixel values to compute a feature representation of the object.
One major drawback of Mask R-CNN is the compute power and processing time it requires to segment images. The GPU we used simply could not handle Tensorflow models with 10,000+ images without running out of memory. For this reason, we used a subset of the dog and cat data. We ended up clustering roughly 8,800 images, where half were dog images and half were cat images.
<br>
<h5>Foreground Feature Representation</h5>
We have implemented various feature extractors based on color and texture, and have experimented with which combination of those feature extractors provide the best clustering result. <br>
<h5>Distance Function</h5>
When determining the similarity of two images, there are a number of distance metrics that could be used. With our feature representation, Euclidean distance is the best option. The results are shown in the next section. <br>
<h5>Clustering Algorithm</h5>
First, we need to decide whether we would cluster the dogs and cats separately or all together.<br>
Next, we would experiment with the clustering methods. We can try both hard and soft clustering. Hard clustering assigns each point to a cluster definitively, soft clustering uses a probabilistic approach to assign probabilities of belonging to each cluster for each point. Both could provide interesting results in this project since we expect hard clustering to give cleaner results faster but soft clustering to perhaps find some insights missing from hard clustering.<br>
Another detail would be what hard clustering algorithm we would utilize. We would decide between centroid-based and density-based, i.e. mean clustering vs. mode clustering. We would figure out these two details by comparing the accuracy of each implementation. <br>
Additionally, we can try varying the value of k in hard clustering. This is key to finding meaningful groups in the dataset. If the clusters are too small, similar images may not be grouped together. On the other, clusters that are too large will group dissimilar images together. We would decide this using the elbow method, trying to find the optimal value of k.

<br><br><br>
<h2>Results</h2>
Using the Mask R-CNN implementation, and updating it to detect cats and dogs only, we can produce the following succesful segmentation:
<br><br>
<div style="text-align: center;">
  <div>
    <img style="height: 200px;" alt="" src="img/mask_rcnn_input.jpg">
    <br>Input to Mask R-CNN
  </div>
</br></br>
  <div>
    <img style="height: 200px;" alt="" src="img/mask_rcnn_output.png">
    <br>Output of Mask R-CNN
  </div>
</div>
<br><br><br>
Here are the results of our proposed system given several input images using different clustering techniques:
<br><br>
<div style="text-align: left;">
  <div>
    <img style="height: 200px;" alt="" src="img/example1.png">
    <br>Example 1
  </div>
</br></br>
  <div>
    <img style="height: 200px;" alt="" src="img/example2.png">
    <br>Example 2
  </div>
</br></br>
  <div>
    <img style="height: 200px;" alt="" src="img/example3.png">
    <br>Example 3
  </div>
<br><br>
  <div>
    <img style="height: 200px;" alt="" src="img/example4.png">
    <br>Example 4
  </div>
</div>

<br><br><br>

<h2>Conclusion/Future Work</h2>
We have quite a few things to achieve for our final project update. The first thing we have to do is replace the clustering input with feature representations of the images to ensure that our feature representations accurately represent the image content we are trying to retrieve. This can also help ensure that the specifications of our clustering algorithm are good. Based on this, we can update our feature representation and clustering accordingly, including running clustering experiments to find optimal parameter values as well as testing various clustering algorithms. We also need to implement an effective way of storing our data such that it can be accessed easily, and information about the closest cluster center and feature representation can be stored. Lastly, we need to connect our project interface to our pipeline so that we can perform end-to-end testing.

<br><br><br>
<h2>References</h2>
<ol>
  <li><a href="https://github.com/matterport/Mask_RCNN">Mask R-CNN</a></li>
  <li><a href="https://www.researchgate.net/publication/220595166_Image_Clustering_using_Color_Texture_and_Shape_Features">CBIR with Color and Texture</a></li>
  <li><a href="https://link.springer.com/chapter/10.1007/978-3-642-27329-2_105">Using Gray Level Co-occurence Matrix</a></li>
</ol>


<br><br>
<!-- Footer -->
  <hr>
  <footer>
  <p>© Hemanth Chittanuru, Kenny Scharm, Raghav Raj Mittal, Sarah Li</p>
  </footer>
</div>
</div>

<br><br>

</body></html>
